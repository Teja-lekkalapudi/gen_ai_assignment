{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe75baa",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a8ddf",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
    "\n",
    "## 1. Introduction: The Inner Monologue\n",
    "\n",
    "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
    "\n",
    "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering. \n",
    "\n",
    "### Why use a \"Dumb\" Model?\n",
    "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
    "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
    "\n",
    "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
    "\n",
    "### Visualizing the Process (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    Input[Question: 5+5*2?]\n",
    "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
    "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
    "    Step1 --> Step2[Step 2: 5+10=15]\n",
    "    Step2 --> Correct[Answer: 15 (Correct)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41e5ba",
   "metadata": {},
   "source": [
    "## 2. Concept: Latent Reasoning\n",
    "\n",
    "Why does this work?\n",
    "Because LLMs are \"Next Token Predictors\".\n",
    "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
    "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
    "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
    "\n",
    "**Writing is Thinking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba92b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3088780",
   "metadata": {},
   "source": [
    "## 3. The Experiment: A Tricky Math Problem\n",
    "\n",
    "Let's try a problem that requires multi-step logic.\n",
    "\n",
    "**Problem:**\n",
    "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a70d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
      "\n",
      "2 cans * 3 tennis balls per can = 6 tennis balls\n",
      "\n",
      "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
      "\n",
      "5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
    "\n",
    "# 1. Standard Prompt (Direct Answer)\n",
    "prompt_standard = f\"Answer this question: {question}\"\n",
    "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_standard).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b696ba6",
   "metadata": {},
   "source": [
    "### Critique\n",
    "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
    "\n",
    "Let's force it to think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd65b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain of Thought (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
      "\n",
      "1. Roger already has 5 tennis balls.\n",
      "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
      "3. Now, we add the tennis balls he already had (5) to the tennis balls he bought (6). 5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "# 2. CoT Prompt (Magic Phrase)\n",
    "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
    "\n",
    "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_cot).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd205672",
   "metadata": {},
   "source": [
    "## 4. Analysis\n",
    "\n",
    "Look at the output. By explicitly breaking it down:\n",
    "1.  \"Roger starts with 5.\"\n",
    "2.  \"2 cans * 3 balls = 6 balls.\"\n",
    "3.  \"5 + 6 = 11.\"\n",
    "\n",
    "The model effectively \"debugs\" its own logic by generating the intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee779f",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1fa7c",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
    "\n",
    "## 1. Introduction: Beyond A -> B\n",
    "\n",
    "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
    "\n",
    "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4371aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a348d6",
   "metadata": {},
   "source": [
    "## 2. Tree of Thoughts (ToT)\n",
    "\n",
    "ToT explores multiple branches before making a decision. \n",
    "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
    "\n",
    "### Implementation\n",
    "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea2d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tree of Thoughts (ToT) Result ---\n",
      "As a child psychologist, I would recommend **Solution 2: Involve your child in the gardening process** as the most sustainable approach to encourage your 5-year-old to eat vegetables. This approach is not bribery, and it has several benefits that make it an effective and long-lasting solution.\n",
      "\n",
      "**Why it's sustainable:**\n",
      "\n",
      "1. **Educational value:** By involving your child in the gardening process, you're teaching them about where food comes from, how it grows, and the process of nurturing and harvesting. This knowledge can help them develop a deeper appreciation for the food they eat.\n",
      "2. **Emotional connection:** When children are involved in the process of growing their own food, they develop an emotional connection with the vegetables. They become invested in the process and feel a sense of pride and ownership when they harvest the vegetables.\n",
      "3. **Increased autonomy:** By letting your child help with watering, weeding, and harvesting, you're giving them a sense of autonomy and control over their food choices. This can help them feel more confident and motivated to try new vegetables.\n",
      "4. **Long-term benefits:** The skills and knowledge your child gains from gardening can benefit them throughout their lives. They may become more interested in cooking and trying new foods, and they may be more likely to make healthy food choices.\n",
      "\n",
      "**Why it's not bribery:**\n",
      "\n",
      "1. **No immediate rewards:** This approach doesn't offer immediate rewards or treats for eating vegetables. Instead, it focuses on the process of growing and harvesting food, which can help your child develop a genuine interest in vegetables.\n",
      "2. **No pressure:** Gardening and cooking together can be a fun and enjoyable experience for both you and your child. It doesn't put pressure on your child to eat vegetables, but rather encourages them to take an active role in the process.\n",
      "3. **Fosters a growth mindset:** By involving your child in the gardening process, you're helping them develop a growth mindset. They learn that trying new foods and exploring new experiences is a natural part of growth and development.\n",
      "\n",
      "Overall, involving your child in the gardening process is a sustainable and effective way to encourage them to eat vegetables. It offers a range of benefits, from educational value to emotional connection, and fosters a growth mindset that can benefit your child throughout their lives.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
    "\n",
    "# Step 1: The Branch Generator\n",
    "prompt_branch = ChatPromptTemplate.from_template(\n",
    "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
    ")\n",
    "\n",
    "branches = RunnableParallel(\n",
    "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
    "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
    "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Step 2: The Judge\n",
    "prompt_judge = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three proposed solutions for: '{problem}'\n",
    "    \n",
    "    1: {sol1}\n",
    "    2: {sol2}\n",
    "    3: {sol3}\n",
    "    \n",
    "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Chain: Input -> Branches -> Judge -> Output\n",
    "tot_chain = (\n",
    "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
    "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]}) \n",
    "    | prompt_judge\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
    "print(tot_chain.invoke(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38579cab",
   "metadata": {},
   "source": [
    "## 3. Graph of Thoughts (GoT)\n",
    "\n",
    "You asked: **\"Where is Graph of Thoughts?\"**\n",
    "\n",
    "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
    "\n",
    "### The Workflow (Writer's Room)\n",
    "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
    "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
    "3.  **Refine:** Polish the Master Plot.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "   Start(Concept) --> A[Draft 1]\n",
    "   Start --> B[Draft 2]\n",
    "   Start --> C[Draft 3]\n",
    "   A & B & C --> Mixer[Aggregator]\n",
    "   Mixer --> Final[Final Story]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Graph of Thoughts (GoT) Result ---\n",
      "\"Echoes of Eternity\" is a heart-pounding, mind-bending thriller that combines the thrill of science fiction, the passion of romance, and the terror of horror. When brilliant physicist Emma Taylor discovers a way to harness the power of quantum entanglements, she unwittingly sends her younger self back in time to the night her high school sweetheart, Matt, died in a tragic accident. As Emma navigates the treacherous consequences of altering the past, she must confront the dark force that drove Matt's death: a malevolent entity that has been awakened by her time traveling. With each attempt to correct the timeline, Emma finds herself reliving the same fateful night, but with a twist: every time, Matt is still alive, and their love is rekindled - but at a terrible cost, as the entity closes in, threatening to destroy not only her future, but the very fabric of time itself.\n"
     ]
    }
   ],
   "source": [
    "# 1. The Generator (Divergence)\n",
    "prompt_draft = ChatPromptTemplate.from_template(\n",
    "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
    ")\n",
    "\n",
    "drafts = RunnableParallel(\n",
    "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
    "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
    "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# 2. The Aggregator (Convergence)\n",
    "prompt_combine = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three movie ideas for the topic '{topic}':\n",
    "    1. Sci-Fi: {draft_scifi}\n",
    "    2. Romance: {draft_romance}\n",
    "    3. Horror: {draft_horror}\n",
    "    \n",
    "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
    "    Write one paragraph.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 3. The Chain\n",
    "got_chain = (\n",
    "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
    "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]}) \n",
    "    | prompt_combine\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
    "print(got_chain.invoke(\"Time Travel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cb724",
   "metadata": {},
   "source": [
    "## 4. Summary & Comparison Table\n",
    "\n",
    "| Method | Structure | Best For... | Cost/Latency |\n",
    "|--------|-----------|-------------|--------------|\n",
    "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
    "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
    "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High | \n",
    "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
    "\n",
    "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780baf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step-by-step reasoning:**\n",
      "\n",
      "1. Calculate the average:\n",
      "   - To find the average, we need to add up all the scores and divide by the number of subjects.\n",
      "   - Math score = 78\n",
      "   - Physics score = 85\n",
      "   - Chemistry score = 91\n",
      "   - Total score = 78 + 85 + 91 = 254\n",
      "   - Number of subjects = 3\n",
      "   - Average = Total score / Number of subjects\n",
      "   - Average = 254 / 3\n",
      "   - Average = 84.67\n",
      "\n",
      "2. Determine the grade:\n",
      "   - We will compare each subject score with the given grade ranges.\n",
      "   - Math score = 78 (Below 90, so it can be either C or B, but since 75-89 is B, it will fall under B)\n",
      "   - Physics score = 85 (Below 90, so it will fall under B)\n",
      "   - Chemistry score = 91 (Above 90, so it will fall under A)\n",
      "   - Since the highest grade is 'A', the overall grade will be A\n",
      "\n",
      "**Final Answer:**\n",
      "Average = 84.67\n",
      "Grade = A\n"
     ]
    }
   ],
   "source": [
    "advanced_prompt = \"\"\"\n",
    "You are an expert problem-solving assistant.\n",
    "\n",
    "TASK:\n",
    "A student scored the following marks:\n",
    "\n",
    "Math = 78\n",
    "Physics = 85\n",
    "Chemistry = 91\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Calculate the average.\n",
    "2. Determine the grade using:\n",
    "   - A: 90+\n",
    "   - B: 75–89\n",
    "   - C: 60–74\n",
    "   - D: below 60\n",
    "3. Explain the steps clearly.\n",
    "4. Return the final answer in structured format.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "Step-by-step reasoning:\n",
    "<steps>\n",
    "\n",
    "Final Answer:\n",
    "Average = ?\n",
    "Grade = ?\n",
    "\"\"\"\n",
    "\n",
    "print(llm.invoke(advanced_prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86d4982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Evaluation Report**\n",
      "\n",
      "**Student Information:** \n",
      "(No student information is provided, so we will refer to the student as 'Student-X')\n",
      "\n",
      "**Subjects:** \n",
      "- Math\n",
      "- Physics\n",
      "- Chemistry\n",
      "\n",
      "**Marks:**\n",
      "\n",
      "| Subject | Marks |\n",
      "|---------|-------|\n",
      "| Math    | 78    |\n",
      "| Physics | 85    |\n",
      "| Chemistry| 91    |\n",
      "\n",
      "**Calculation Steps:**\n",
      "\n",
      "1. Add up all the marks: 78 + 85 + 91 = 254\n",
      "2. Count the number of subjects: 3\n",
      "3. Calculate the average by dividing the total marks by the number of subjects: 254 ÷ 3 = 84.67\n",
      "\n",
      "**Structured Output:**\n",
      "\n",
      "| **Subject** | **Marks** | **Average** |\n",
      "|-------------|----------|-------------|\n",
      "| Math        | 78       |             |\n",
      "| Physics     | 85       |             |\n",
      "| Chemistry   | 91       |             |\n",
      "| **Average** |          | 84.67       |\n",
      "\n",
      "**Grade Assignment:**\n",
      "\n",
      "Based on the average, a grade can be assigned as follows:\n",
      "\n",
      "- A: 90-100 (Excellent)\n",
      "- B: 80-89 (Good)\n",
      "- C: 70-79 (Fair)\n",
      "- D: 60-69 (Pass)\n",
      "- F: Below 60 (Fail)\n",
      "\n",
      "Given the average of 84.67, the student can be assigned a grade of **B** (Good).\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "Based on the evaluation, the student has performed well in Chemistry and Physics, but needs improvement in Math to achieve a higher grade. It is recommended that the student focuses on improving their Math skills to achieve an overall better performance.\n"
     ]
    }
   ],
   "source": [
    "# LCLL VERSION \n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert academic evaluator.\n",
    "\n",
    "Calculate the average and assign a grade.\n",
    "\n",
    "Marks:\n",
    "Math = {math}\n",
    "Physics = {physics}\n",
    "Chemistry = {chemistry}\n",
    "\n",
    "Explain steps and return structured output.\n",
    "\"\"\")\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"math\": 78,\n",
    "    \"physics\": 85,\n",
    "    \"chemistry\": 91\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
